{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dba1d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[i 0528 10:44:22.884334 60 log.cc:351] Load log_sync: 1\u001b[m\n",
      "\u001b[38;5;2m[i 0528 10:44:22.953795 60 compiler.py:956] Jittor(1.3.9.14) src: /home/xaMars/anaconda3/envs/jittor/lib/python3.7/site-packages/jittor\u001b[m\n",
      "\u001b[38;5;2m[i 0528 10:44:22.966998 60 compiler.py:957] g++ at /usr/bin/g++(9.4.0)\u001b[m\n",
      "\u001b[38;5;2m[i 0528 10:44:22.968677 60 compiler.py:958] cache_path: /home/xaMars/.cache/jittor/jt1.3.9/g++9.4.0/py3.7.12/Linux-5.15.0-1xe9/IntelRXeonRGolxc9/c208/default\u001b[m\n",
      "\u001b[38;5;2m[i 0528 10:44:23.027714 60 install_cuda.py:96] cuda_driver_version: [12, 2]\u001b[m\n",
      "\u001b[38;5;2m[i 0528 10:44:23.032215 60 install_cuda.py:82] needed restart but not /home/xaMars/anaconda3/envs/jittor/bin/python ['-m', 'ipykernel_launcher', '--f=/run/user/1002/jupyter/runtime/kernel-v306da857581d4e14a76c3de7f57a4a3e8686e81af.json'], you can ignore this warning.\u001b[m\n",
      "\u001b[38;5;2m[i 0528 10:44:23.047910 60 __init__.py:412] Found /home/xaMars/.cache/jittor/jtcuda/cuda12.2_cudnn8_linux/bin/nvcc(12.2.140) at /home/xaMars/.cache/jittor/jtcuda/cuda12.2_cudnn8_linux/bin/nvcc.\u001b[m\n",
      "\u001b[38;5;2m[i 0528 10:44:23.154663 60 __init__.py:412] Found gdb(20.04.1) at /usr/bin/gdb.\u001b[m\n",
      "\u001b[38;5;2m[i 0528 10:44:23.173346 60 __init__.py:412] Found addr2line(2.34) at /usr/bin/addr2line.\u001b[m\n",
      "\u001b[38;5;2m[i 0528 10:44:23.390318 60 compiler.py:1013] cuda key:cu12.2.140_sm_89\u001b[m\n",
      "\u001b[38;5;2m[i 0528 10:44:23.469628 60 compiler.py:34] Create cache dir: /home/xaMars/.cache/jittor/jt1.3.9/g++9.4.0/py3.7.12/Linux-5.15.0-1xe9/IntelRXeonRGolxc9/c208/default/cu12.2.140_sm_89\u001b[m\n",
      "\u001b[38;5;2m[i 0528 10:44:23.473787 60 compiler.py:34] Create cache dir: /home/xaMars/.cache/jittor/jt1.3.9/g++9.4.0/py3.7.12/Linux-5.15.0-1xe9/IntelRXeonRGolxc9/c208/default/cu12.2.140_sm_89/jit\u001b[m\n",
      "\u001b[38;5;2m[i 0528 10:44:23.475405 60 compiler.py:34] Create cache dir: /home/xaMars/.cache/jittor/jt1.3.9/g++9.4.0/py3.7.12/Linux-5.15.0-1xe9/IntelRXeonRGolxc9/c208/default/cu12.2.140_sm_89/obj_files\u001b[m\n",
      "\u001b[38;5;2m[i 0528 10:44:23.476417 60 compiler.py:34] Create cache dir: /home/xaMars/.cache/jittor/jt1.3.9/g++9.4.0/py3.7.12/Linux-5.15.0-1xe9/IntelRXeonRGolxc9/c208/default/cu12.2.140_sm_89/gen\u001b[m\n",
      "\u001b[38;5;2m[i 0528 10:44:23.477802 60 compiler.py:34] Create cache dir: /home/xaMars/.cache/jittor/jt1.3.9/g++9.4.0/py3.7.12/Linux-5.15.0-1xe9/IntelRXeonRGolxc9/c208/default/cu12.2.140_sm_89/tmp\u001b[m\n",
      "\u001b[38;5;2m[i 0528 10:44:23.478775 60 compiler.py:34] Create cache dir: /home/xaMars/.cache/jittor/jt1.3.9/g++9.4.0/py3.7.12/Linux-5.15.0-1xe9/IntelRXeonRGolxc9/c208/default/cu12.2.140_sm_89/checkpoints\u001b[m\n",
      "\u001b[38;5;2m[i 0528 10:44:34.999673 60 __init__.py:227] Total mem: 125.54GB, using 16 procs for compiling.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling jittor_core(150/151) used: 18.930s eta: 0.126s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[i 0528 10:45:07.290493 60 jit_compiler.cc:28] Load cc_path: /usr/bin/g++\u001b[m\n",
      "\u001b[38;5;2m[i 0528 10:45:07.448202 60 init.cc:63] Found cuda archs: [89,]\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling jittor_core(151/151) used: 31.722s eta: 0.000s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xaMars/anaconda3/envs/jittor/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[38;5;2m[i 0528 10:45:08.913833 60 compile_extern.py:388] Downloading cutt...\u001b[m\n",
      "\u001b[38;5;2m[i 0528 10:45:08.925537 60 compile_extern.py:401] installing cutt...\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling libcutt(8/9) used: 6.194s eta: 0.774s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[i 0528 10:45:20.467114 60 compiler.py:34] Create cache dir: /home/xaMars/.cache/jittor/jt1.3.9/g++9.4.0/py3.7.12/Linux-5.15.0-1xe9/IntelRXeonRGolxc9/c208/default/cu12.2.140_sm_89/custom_ops\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling libcutt(9/9) used: 11.402s eta: 0.000s\n",
      "Compiling gen_ops_mkl_conv_backward_w_mkl_matmul_mkl_conv_mk___hashd7fd3e(6/7) used: 3.207s eta: 0.534s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[i 0528 10:45:26.144889 60 compiler.py:34] Create cache dir: /home/xaMars/.cache/jittor/jt1.3.9/g++9.4.0/py3.7.12/Linux-5.15.0-1xe9/IntelRXeonRGolxc9/c208/default/cu12.2.140_sm_89/cuda\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling gen_ops_mkl_conv_backward_w_mkl_matmul_mkl_conv_mk___hashd7fd3e(7/7) used: 3.497s eta: 0.000s\n",
      "Compiling gen_ops_cub_cumsum_cub_test_cub_argsort_cub_where____hasha59b7d(6/6) used: 2.943s eta: 0.000s\n",
      "Compiling gen_ops_cublas_test_cublas_batched_matmul_cublas_m___hash88a5d2(8/8) used: 2.613s eta: 0.000s\n",
      "Compiling gen_ops_cudnn_conv3d_backward_x_cudnn_rnn_backward___hasha84772(16/16) used: 5.949s eta: 0.000s\n",
      "Compiling gen_ops_cusparse_spmmcoo_cusparse_spmmcsr(5/5) used: 2.121s eta: 0.000s\n",
      "\n",
      "import jittor as jt\n",
      "from jittor import init\n",
      "from jittor import nn\n",
      "\n",
      "class AlexNet(nn.Module):\n",
      "\n",
      "    def __init__(self, num_classes=1000):\n",
      "        super(AlexNet, self).__init__()\n",
      "        self.features = nn.Sequential(nn.Conv(3, 64, 11, stride=4, padding=2), nn.ReLU(), nn.Pool(3, stride=2, op='maximum'), nn.Conv(64, 192, 5, padding=2), nn.ReLU(), nn.Pool(3, stride=2, op='maximum'), nn.Conv(192, 384, 3, padding=1), nn.ReLU(), nn.Conv(384, 256, 3, padding=1), nn.ReLU(), nn.Conv(256, 256, 3, padding=1), nn.ReLU(), nn.Pool(3, stride=2, op='maximum'))\n",
      "        self.classifier = nn.Sequential(nn.Dropout(), nn.Linear(((256 * 6) * 6), 4096), nn.ReLU(), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(), nn.Linear(4096, num_classes))\n",
      "\n",
      "    def execute(self, x):\n",
      "        x = self.features(x)\n",
      "        x = jt.flatten(x, start_dim=1)\n",
      "        x = self.classifier(x)\n",
      "        return x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from jittor.utils.pytorch_converter import convert\n",
    "\n",
    "pytorch_code=\"\"\"\n",
    "from torch import nn\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\"\"\"\n",
    "\n",
    "jittor_code = convert(pytorch_code)\n",
    "print(jittor_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e76b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jittor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
